{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Flatten, BatchNormalization,Normalization,Dropout\n",
    "from keras.initializers import HeNormal,lecun_normal\n",
    "from keras.activations import swish, selu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si TensorFlow detecta la GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"¡GPU encontrada!\")\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "else:\n",
    "    print(\"¡GPU no encontrada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practica cómo entrenar una red neuronal profunda con el conjunto de datos de imágenes CIFAR10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A\n",
    "El conjunto de datos está compuesto por 60.000 imágenes en color de 32 x 32 píxe­les (50.000 para el entrenamiento, 5.000 para la validación y 5.000 para las pruebas) con 10 clases. Puedes cargarlo con tf.keras.datasets.cifar10.load_data(). Muestra una de la imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionar todos los datos para dividir posteriormente\n",
    "x_total = np.concatenate((x_train, x_test))\n",
    "y_total = np.concatenate((y_train, y_test))\n",
    "\n",
    "# Dividir el conjunto completo en 50,000 para entrenamiento y 10,000 para validación + test\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_total, y_total, test_size=10000, random_state=42)\n",
    "\n",
    "# Dividir los 10,000 restantes en 5,000 para validación y 5,000 para test\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=5000, random_state=42)\n",
    "\n",
    "# Imprimir las formas resultantes\n",
    "print(\"Forma de x_train:\", x_train.shape)\n",
    "print(\"Forma de x_val:\", x_val.shape)\n",
    "print(\"Forma de x_test:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.5, 1.5))\n",
    "plt.imshow(x_train[0])\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B\n",
    "Crea una RNP con 20 capas ocultas de 100 neuronas cada una (son demasiadas,\n",
    "pero esa es la gracia del ejercicio). \n",
    "\n",
    "Utiliza la inicialización He y la función de activa­ción Swish. \n",
    "\n",
    "Antes de las 20 capas tendras que añadir una capa Input y una capa Flatten y despúes una capa de de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(32, 32,3)))\n",
    "model.add(Flatten())\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation=swish, kernel_initializer=HeNormal()))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C\n",
    "Elige una tasa de aprendizaje adecuada para la optimización Nadam.\n",
    "\n",
    "Para probar los diferentes modelos haz un bucle (después de cada entrenamiento evalua X_train y X_valid y guarda en un diccionario para \"Red neuronal normal\")\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "\n",
    "[{\n",
    "\n",
    "    'Red neuronal normal': {\n",
    "\n",
    "        'Tasa de aprendizaje': X,\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': X,\n",
    "\n",
    "        'Validación sparse categorical accuracy': X,\n",
    "\n",
    "        'Tiempo total (s)': X\n",
    "\n",
    "    }\n",
    "    \n",
    "}] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates=[1e-4,3e-4,7e-4,1e-3,3e-3,7e-3]\n",
    "dicc=[]\n",
    "\n",
    "mejor_acc_val=0\n",
    "mejor_modelo=None\n",
    "    \n",
    "i=0\n",
    "for idx, lr in enumerate(learning_rates,start=0):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rates[i])\n",
    "    \n",
    "\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    history=model.fit(x_train, y_train, epochs=30, validation_data=(x_val, y_val))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "\n",
    "    train_acc= history.history['sparse_categorical_accuracy'][-1]\n",
    "    val_acc = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "    \n",
    "    if val_acc > mejor_acc_val:\n",
    "        mejor_acc_train=train_acc\n",
    "        mejor_acc_val = val_acc\n",
    "        mejor_tiempo=training_time\n",
    "\n",
    "        mejor_modelo = model\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_lr=mejor_modelo.optimizer.learning_rate.numpy()\n",
    "print(mejor_lr)\n",
    "print(mejor_acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_SC_accuracy = mejor_modelo.evaluate(x_train, y_train)\n",
    "print(\"Pérdida:\", train_loss, \"Precisión:\", train_SC_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_SC_accuracy = mejor_modelo.evaluate(x_val, y_val)\n",
    "print(\"Pérdida:\", val_loss, \"Precisión:\", val_SC_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_SC_accuracy = mejor_modelo.evaluate(x_test, y_test)\n",
    "print(\"Pérdida:\", test_loss, \"Precisión:\", test_SC_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc.append({\n",
    "    'Red neuronal normal': {\n",
    "\n",
    "        'Tasa de aprendizaje': mejor_modelo.optimizer.learning_rate.numpy(),\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': train_SC_accuracy,\n",
    "\n",
    "        'Validación sparse categorical accuracy': test_SC_accuracy,\n",
    "\n",
    "        'Tiempo total (s)': mejor_tiempo\n",
    "\n",
    "    }\n",
    "})\n",
    "for entrada in dicc:\n",
    "    print(entrada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D\n",
    " Una vez elegida la tasa de aprendizaje entrena el modelo usando la detención temprana (ahora que tienes detención temprana aumenta el número de epochs).\n",
    "\n",
    " Añade los resultados al diccionario anterior, hay que añadir:\n",
    "\n",
    "[{\n",
    "\n",
    "    'Red neuronal normal con detención temprana': {\n",
    "\n",
    "        'Tasa de aprendizaje': X,\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': X,\n",
    "\n",
    "        'Validación sparse categorical accuracy': X,\n",
    "\n",
    "        'Tiempo total (s)': X,\n",
    "\n",
    "        'Mejor época': X\n",
    "\n",
    "    }\n",
    "    \n",
    "}] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=float(mejor_lr))\n",
    "    \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "model_opti = Sequential()\n",
    "model_opti.add(Input(shape=(32, 32,3)))\n",
    "model_opti.add(Flatten())\n",
    "for _ in range(20):\n",
    "    model_opti.add(Dense(100, activation=swish, kernel_initializer=HeNormal()))\n",
    "model_opti.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "model_opti.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=optimizer,\n",
    "            metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "history=model_opti.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val),callbacks=early_stopping_cb)\n",
    "\n",
    "train_acc= history.history['sparse_categorical_accuracy'][-1]\n",
    "val_acc = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "   \n",
    "end_time = time.time()\n",
    "\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_acc= max(history.history['sparse_categorical_accuracy'])\n",
    "best_epoch_index = history.history['sparse_categorical_accuracy'].index(best_epoch_acc)\n",
    "\n",
    "\n",
    "dicc.append({\n",
    "    'Red neuronal normal con detención temprana': {\n",
    "\n",
    "        'Tasa de aprendizaje': best_model['LR'],\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': history.history['sparse_categorical_accuracy'][-1],\n",
    "\n",
    "        'Validación sparse categorical accuracy': history.history['val_sparse_categorical_accuracy'][-1],\n",
    "\n",
    "        'Tiempo total (s)': training_time,\n",
    "\n",
    "        'Mejor época': best_epoch_index\n",
    "\n",
    "    }\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entrada in dicc:\n",
    "    print(entrada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E\n",
    "Ahora, prueba a añadir normalización de lotes y repite C y D con esta nueva red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "\n",
    "modelo_lotes = Sequential()\n",
    "modelo_lotes.add(Input(shape=(32, 32,3)))\n",
    "modelo_lotes.add(Flatten())\n",
    "\n",
    "for _ in range(20):\n",
    "    modelo_lotes.add(BatchNormalization())\n",
    "    modelo_lotes.add(Dense(100, activation=swish, kernel_initializer=HeNormal()))\n",
    "modelo_lotes.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates=[1e-4,3e-4,7e-4,1e-3,3e-3,7e-3]\n",
    "entries=[]\n",
    "histories=[]\n",
    "models_lotes=[]\n",
    "\n",
    "    \n",
    "i=0\n",
    "for idx, lr in enumerate(learning_rates,start=0):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rates[i])\n",
    "    \n",
    "\n",
    "    i+=1\n",
    "    modelo_lotes.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    history=modelo_lotes.fit(x_train, y_train, epochs=45, validation_data=(x_val, y_val))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    entries.append({'ID':idx,\n",
    "                 'LR':lr,\n",
    "                 'sparse categorical accuracy training':history.history['sparse_categorical_accuracy'][-1],\n",
    "                 'sparse categorical accuracy validation':history.history['val_sparse_categorical_accuracy'][-1],\n",
    "                 'Tiempo Total(s)':training_time\n",
    "                   })\n",
    "    models_lotes.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_lotes = max(entries, key=lambda x: x['sparse categorical accuracy validation'])\n",
    "print(best_model_lotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc.append({\n",
    "    'Red neuronal por lotes': {\n",
    "\n",
    "        'Tasa de aprendizaje': best_model_lotes['LR'],\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': best_model_lotes['sparse categorical accuracy training'],\n",
    "\n",
    "        'Validación sparse categorical accuracy': best_model_lotes['sparse categorical accuracy validation'],\n",
    "\n",
    "        'Tiempo total (s)': best_model_lotes['Tiempo Total(s)']\n",
    "\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entrada in dicc:\n",
    "    print(entrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_lotes = tf.keras.optimizers.Nadam(learning_rate=best_model_lotes['LR'])\n",
    "    \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "model_lotes_opti = Sequential()\n",
    "model_lotes_opti.add(Input(shape=(32, 32,3)))\n",
    "model_lotes_opti.add(Flatten())\n",
    "for _ in range(20):\n",
    "    model_lotes_opti.add(BatchNormalization())\n",
    "    model_lotes_opti.add(Dense(100, activation=swish, kernel_initializer=HeNormal()))\n",
    "model_lotes_opti.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "model_lotes_opti.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                         optimizer=optimizer_lotes,\n",
    "                         metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "history_lotes=model_lotes_opti.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val),callbacks=early_stopping_cb)\n",
    "\n",
    "   \n",
    "end_time = time.time()\n",
    "\n",
    "training_time_lotes = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_acc= max(history_lotes.history['sparse_categorical_accuracy'])\n",
    "best_epoch_index = history_lotes.history['sparse_categorical_accuracy'].index(best_epoch_acc)\n",
    "\n",
    "\n",
    "dicc.append({\n",
    "    'Red neuronal por lotes con detención temprana': {\n",
    "\n",
    "        'Tasa de aprendizaje': best_model_lotes['LR'],\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': history_lotes.history['sparse_categorical_accuracy'][-1],\n",
    "\n",
    "        'Validación sparse categorical accuracy': history_lotes.history['val_sparse_categorical_accuracy'][-1],\n",
    "\n",
    "        'Tiempo total (s)': training_time_lotes,\n",
    "\n",
    "        'Mejor época': best_epoch_index\n",
    "\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entrada in dicc:\n",
    "    print(entrada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F\n",
    "Prueba a sustituir la normalización de lotes por la activación SELU y haz los ajustes necesarios para garantizar que la red se autonormaliza (es decir, tienes que estandarizar los datos antes de empezar).\n",
    "\n",
    "En este caso prueba a estandarizar manualmentes, es decir restando la media y dividiendo por la desviación standard.\n",
    "\n",
    "Usa la inicialización LeCun normal.\n",
    "\n",
    "Repite C y D con esta nueva red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
    "x_val_norm = (x_val - np.min(x_val)) / (np.max(x_val) - np.min(x_val))\n",
    "x_test_norm = (x_test - np.min(x_test)) / (np.max(x_test) - np.min(x_test))\n",
    "\n",
    "y_train_norm = (y_train - np.min(y_train)) / (np.max(y_train) - np.min(y_train))\n",
    "y_val_norm = (y_val - np.min(y_val)) / (np.max(y_val) - np.min(y_val))\n",
    "y_test_norm = (y_test - np.min(y_test)) / (np.max(y_test) - np.min(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "\n",
    "model_selu = Sequential()\n",
    "model_selu.add(Input(shape=(32, 32,3)))\n",
    "model_selu.add(Flatten())\n",
    "for _ in range(20):\n",
    "    model_selu.add(Dense(100, activation=selu, kernel_initializer=lecun_normal()))\n",
    "model_selu.add(Dense(10, activation=\"softmax\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates=[1e-4,3e-4,7e-4,1e-3,3e-3,7e-3]\n",
    "entries=[]\n",
    "histories_selu=[]\n",
    "models_selu=[]\n",
    "\n",
    "    \n",
    "i=0\n",
    "for idx, lr in enumerate(learning_rates,start=0):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rates[i])\n",
    "    \n",
    "\n",
    "    i+=1\n",
    "    model_selu.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    history=model_selu.fit(x_train, y_train, epochs=45, validation_data=(x_val, y_val))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    entries.append({'ID':idx,\n",
    "                 'LR':lr,\n",
    "                 'sparse categorical accuracy training':history.history['sparse_categorical_accuracy'][-1],\n",
    "                 'sparse categorical accuracy validation':history.history['val_sparse_categorical_accuracy'][-1],\n",
    "                 'Tiempo Total(s)':training_time\n",
    "                   })\n",
    "    models_selu.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_selu = max(entries, key=lambda x: x['sparse categorical accuracy validation'])\n",
    "print(best_model_selu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc.append({\n",
    "    'Red neuronal selu': {\n",
    "\n",
    "        'Tasa de aprendizaje': best_model_selu['LR'],\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': best_model_selu['sparse categorical accuracy training'],\n",
    "\n",
    "        'Validación sparse categorical accuracy': best_model_selu['sparse categorical accuracy validation'],\n",
    "\n",
    "        'Tiempo total (s)': best_model_selu['Tiempo Total(s)']\n",
    "\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in dicc:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_selu = tf.keras.optimizers.Nadam(learning_rate=best_model_selu['LR'])\n",
    "    \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_selu_opti = Sequential()\n",
    "model_selu_opti.add(Input(shape=(32, 32,3)))\n",
    "model_selu_opti.add(Flatten())\n",
    "for _ in range(20):\n",
    "    model_selu_opti.add(Dense(100, activation=selu, kernel_initializer=lecun_normal()))\n",
    "model_selu_opti.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model_selu_opti.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                         optimizer=optimizer_selu,\n",
    "                         metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "history_selu=model_selu_opti.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val),callbacks=early_stopping_cb)\n",
    "\n",
    "   \n",
    "end_time = time.time()\n",
    "\n",
    "training_time_selu = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_acc= max(history_selu.history['sparse_categorical_accuracy'])\n",
    "best_epoch_index = history_selu.history['sparse_categorical_accuracy'].index(best_epoch_acc)\n",
    "\n",
    "\n",
    "dicc.append({\n",
    "    'Red neuronal selu con detención temprana': {\n",
    "\n",
    "        'Tasa de aprendizaje': best_model_selu['LR'],\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': history_selu.history['sparse_categorical_accuracy'][-1],\n",
    "\n",
    "        'Validación sparse categorical accuracy': history_selu.history['val_sparse_categorical_accuracy'][-1],\n",
    "\n",
    "        'Tiempo total (s)': training_time_selu,\n",
    "\n",
    "        'Mejor época': best_epoch_index\n",
    "\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in dicc:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G\n",
    "Prueba ahora a regularizar el modelo anterior añadiendo una capa dropout antes de la última capa (estandariza manualmente como en el punto anterior).\n",
    "\n",
    "Repite C y D con esta nueva red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "\n",
    "model_dropout = Sequential()\n",
    "model_dropout.add(Input(shape=(32, 32,3)))\n",
    "model_dropout.add(Flatten())\n",
    "for _ in range(20):\n",
    "    model_dropout.add(Dense(100, activation=selu, kernel_initializer=lecun_normal()))\n",
    "model_dropout.add(Dropout(rate=0.2))\n",
    "model_dropout.add(Dense(10, activation=\"softmax\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates=[1e-4,3e-4,7e-4,1e-3,3e-3,7e-3]\n",
    "entries=[]\n",
    "histories_dropout=[]\n",
    "models_dropout=[]\n",
    "\n",
    "    \n",
    "i=0\n",
    "for idx, lr in enumerate(learning_rates,start=0):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rates[i])\n",
    "    \n",
    "\n",
    "    i+=1\n",
    "    model_dropout.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    history=model_dropout.fit(x_train, y_train, epochs=45, validation_data=(x_val, y_val))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    entries.append({'ID':idx,\n",
    "                 'LR':lr,\n",
    "                 'sparse categorical accuracy training':history.history['sparse_categorical_accuracy'][-1],\n",
    "                 'sparse categorical accuracy validation':history.history['val_sparse_categorical_accuracy'][-1],\n",
    "                 'Tiempo Total(s)':training_time\n",
    "                   })\n",
    "    models_selu.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dropout= max(entries, key=lambda x: x['sparse categorical accuracy validation'])\n",
    "print(best_model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc.append({\n",
    "    'Red neuronal dropout': {\n",
    "\n",
    "        'Tasa de aprendizaje': best_model_dropout['LR'],\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': best_model_dropout['sparse categorical accuracy training'],\n",
    "\n",
    "        'Validación sparse categorical accuracy': best_model_dropout['sparse categorical accuracy validation'],\n",
    "\n",
    "        'Tiempo total (s)': best_model_dropout['Tiempo Total(s)']\n",
    "\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in dicc:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dropout = tf.keras.optimizers.Nadam(learning_rate=best_model_dropout['LR'])\n",
    "    \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_dropout_opti = Sequential()\n",
    "model_dropout_opti.add(Input(shape=(32, 32,3)))\n",
    "model_dropout_opti.add(Flatten())\n",
    "for _ in range(20):\n",
    "    model_dropout_opti.add(Dense(100, activation=selu, kernel_initializer=lecun_normal()))\n",
    "model_dropout_opti.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model_dropout_opti.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                         optimizer=optimizer_dropout,\n",
    "                         metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "history_dropout=model_dropout_opti.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val),callbacks=early_stopping_cb)\n",
    "\n",
    "   \n",
    "end_time = time.time()\n",
    "\n",
    "training_time_dropout = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_acc= max(history_dropout.history['sparse_categorical_accuracy'])\n",
    "best_epoch_index_dropout = history_dropout.history['sparse_categorical_accuracy'].index(best_epoch_acc)\n",
    "\n",
    "dicc.append({\n",
    "    'Red neuronal dropout con detención temprana': {\n",
    "\n",
    "        'Tasa de aprendizaje': best_model_dropout['LR'],\n",
    "\n",
    "        'Entrenamiento sparse categorical accuracy': history_dropout.history['sparse_categorical_accuracy'][-1],\n",
    "\n",
    "        'Validación sparse categorical accuracy': history_dropout.history['val_sparse_categorical_accuracy'][-1],\n",
    "\n",
    "        'Tiempo total (s)': training_time_dropout,\n",
    "\n",
    "        'Mejor época': best_epoch_index\n",
    "\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in dicc:\n",
    "    print(entry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IABD3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
