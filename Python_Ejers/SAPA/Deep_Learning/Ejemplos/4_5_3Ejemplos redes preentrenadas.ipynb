{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJEMPLOS DE REDES PREENTRENADAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso1: Importar librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 19:50:35.372265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Preparación de Datos\n",
    "Fashion-MNIST es un conjunto de datos de imágenes de artículos de Zalando que consta de un conjunto de entrenamiento de 60000 ejemplos y un conjunto de prueba de 10000 ejemplos. Cada ejemplo es una imagen en escala de grises de 28x28, asociada con una etiqueta de 10 clases diferentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Construir una red neuronal convolucional para Fashion MNIST \n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso3: División de Datos\n",
    "\n",
    "Vamos a separar fashion MNIST en dos conjuntos de entrenamiento:\n",
    "* `X_train_A`: todas las imagenes excepto T-shirts/tops y pullovers (clases 0 y 2).\n",
    "* `X_train_B`: un conjunto pequeño con las 200 primeras imagenes de T-shirts/tops y pullovers.\n",
    "\n",
    "El conjunto de validación y el conjunto de pruebas también se dividen de esta manera, pero sin restringir la cantidad de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cargar los datos de entrenamiento y test de Fashion MNIST \n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "# Se recuperan los ultimos 5000 datos de entrenamiento para usarlos como datos de validacion\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "# Procedemos a normalizar los datos. Se divide por 255 para que los valores esten entre 0 y 1. Esto se hace con el fin de que el entrenamiento sea mas rapido.\n",
    "# Para los datos de validacion y test no se hace esto, ya que no se deben modificar los datos de validacion y test.\n",
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los nombres de las clases del dataset\n",
    "class_names = [\"T-shirt/top\",\"Trouser\",\"Pullover\", \"Dress\",\"Coat\",\"Sandal\",\"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 5, 6, 7, 8, 9]\n",
      "[1, 3, 4, 5, 6, 7, 8, 9]\n",
      "[1, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Se definen las dos clases que se van a usar para el problema de claseificacion binaria\n",
    "pos_class_id = class_names.index(\"Pullover\") #2\n",
    "neg_class_id = class_names.index(\"T-shirt/top\") #0\n",
    "\n",
    "# La función split_dataset es una función diseñada para dividir un conjunto de datos en dos subconjuntos,\n",
    "# A y B, donde el subconjunto A está destinado a una tarea de clasificación multiclase con 8 clases, \n",
    "# y el subconjunto B está destinado a una tarea de clasificación binaria.\n",
    "def split_dataset(X, y):\n",
    "    # y_for_B -> Crear una máscara booleana para identificar las clases positivas y negativas en y\n",
    "    # y_A -> Crear subconjunto A excluyendo ejemplos de clases positivas y negativas\n",
    "    # y_B -> Crear subconjunto B con clases positivas codificadas como 1 y otras clases como 0\n",
    "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
    "    y_A = y[~y_for_B]\n",
    "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
    "\n",
    "    # Reetiqueta clases en el subconjunto A para que vayan de 0 a 7\n",
    "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
    "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
    "        y_A[y_A == old_class_id] = new_class_id\n",
    "    # Devuelve tupla con subconjunto A (X, y_A) y subconjunto B (X, y_B)\n",
    "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
    "\n",
    "#  dividen los conjuntos de datos originales en subconjuntos A y B, y limitan el tamaño del subconjunto B de entrenamiento a 200 ejemplos\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4 y 5: Creación y entrenamiento de los modelos\n",
    "\n",
    "En este caso se van a crear dos modelos A y B. Se crea el primer modelo A, se entrena y se guarda. Una vez guardado, se crea el segundo modelo. Para la creación de este segundo modelo vamos a mostrar dos opciones. La primera creando un segundo modelo sin transferencia, para ver los resultados. La segunda opción será crear un segundo modelo B con transferencia de capas entrenadas del modelo A. De esta forma podremos comparar la eficiencia de utilizar modelos preentrenados para crear parte de nuestro modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creamos el primer modelo de tipo secuencial - modelo A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiea de sesión y establece semilla aleatoria de tamaño 42\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#Se define el modelo A que es el modelo de clasificación multiclase\n",
    "# Se crea un modelo secuencial con 4 capas densas, la primera capa es la capa de entrada que recibe un tensor de 28x28 y lo aplana\n",
    "# Las siguientes 3 capas son capas densas con 100 neuronas cada una, función de activación relu y kernel_initializer he_normal\n",
    "model_A = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(28, 28)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(8, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Se compila el modelo A con la función de pérdida sparse_categorical_crossentropy, el optimizador SGD con learning_rate=0.001 y la métrica accuracy\n",
    "model_A.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9128 - loss: 0.2526 - val_accuracy: 0.9030 - val_loss: 0.2686\n",
      "Epoch 2/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9143 - loss: 0.2494 - val_accuracy: 0.9037 - val_loss: 0.2661\n",
      "Epoch 3/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9154 - loss: 0.2464 - val_accuracy: 0.9042 - val_loss: 0.2638\n",
      "Epoch 4/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9162 - loss: 0.2435 - val_accuracy: 0.9057 - val_loss: 0.2616\n",
      "Epoch 5/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9171 - loss: 0.2408 - val_accuracy: 0.9065 - val_loss: 0.2595\n",
      "Epoch 6/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9179 - loss: 0.2383 - val_accuracy: 0.9080 - val_loss: 0.2575\n",
      "Epoch 7/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9190 - loss: 0.2358 - val_accuracy: 0.9090 - val_loss: 0.2556\n",
      "Epoch 8/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9199 - loss: 0.2334 - val_accuracy: 0.9090 - val_loss: 0.2537\n",
      "Epoch 9/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9204 - loss: 0.2311 - val_accuracy: 0.9103 - val_loss: 0.2520\n",
      "Epoch 10/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9208 - loss: 0.2289 - val_accuracy: 0.9103 - val_loss: 0.2503\n",
      "Epoch 11/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9214 - loss: 0.2268 - val_accuracy: 0.9108 - val_loss: 0.2486\n",
      "Epoch 12/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9222 - loss: 0.2247 - val_accuracy: 0.9120 - val_loss: 0.2471\n",
      "Epoch 13/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9233 - loss: 0.2228 - val_accuracy: 0.9120 - val_loss: 0.2456\n",
      "Epoch 14/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9239 - loss: 0.2209 - val_accuracy: 0.9118 - val_loss: 0.2442\n",
      "Epoch 15/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9247 - loss: 0.2190 - val_accuracy: 0.9123 - val_loss: 0.2428\n",
      "Epoch 16/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9255 - loss: 0.2172 - val_accuracy: 0.9130 - val_loss: 0.2415\n",
      "Epoch 17/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9259 - loss: 0.2155 - val_accuracy: 0.9143 - val_loss: 0.2402\n",
      "Epoch 18/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9267 - loss: 0.2138 - val_accuracy: 0.9145 - val_loss: 0.2389\n",
      "Epoch 19/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9272 - loss: 0.2121 - val_accuracy: 0.9148 - val_loss: 0.2377\n",
      "Epoch 20/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9280 - loss: 0.2105 - val_accuracy: 0.9158 - val_loss: 0.2366\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20, validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(\"Resultados/my_model_A.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Se crea el segundo modelo B de tipo secuencial. 2 opciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Opcion1: Modelo B sin transferencia\n",
    "A continuación entrenamos y evaluamos el modelo B sin utilizar el modelo A para trasnferir los conocimientos previos del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2576 - loss: 0.8675 - val_accuracy: 0.3116 - val_loss: 0.7882\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3031 - loss: 0.8026 - val_accuracy: 0.3947 - val_loss: 0.7417\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4176 - loss: 0.7488 - val_accuracy: 0.5292 - val_loss: 0.7015\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5694 - loss: 0.7027 - val_accuracy: 0.6360 - val_loss: 0.6655\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6705 - loss: 0.6614 - val_accuracy: 0.7310 - val_loss: 0.6329\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7603 - loss: 0.6242 - val_accuracy: 0.7883 - val_loss: 0.6036\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8225 - loss: 0.5910 - val_accuracy: 0.8309 - val_loss: 0.5776\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8773 - loss: 0.5618 - val_accuracy: 0.8526 - val_loss: 0.5547\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8924 - loss: 0.5356 - val_accuracy: 0.8615 - val_loss: 0.5339\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9068 - loss: 0.5118 - val_accuracy: 0.8714 - val_loss: 0.5152\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9243 - loss: 0.4903 - val_accuracy: 0.8803 - val_loss: 0.4981\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9124 - loss: 0.4709 - val_accuracy: 0.8892 - val_loss: 0.4827\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9151 - loss: 0.4536 - val_accuracy: 0.8922 - val_loss: 0.4686\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9220 - loss: 0.4378 - val_accuracy: 0.8971 - val_loss: 0.4553\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9328 - loss: 0.4230 - val_accuracy: 0.9001 - val_loss: 0.4429\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9328 - loss: 0.4093 - val_accuracy: 0.9021 - val_loss: 0.4313\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9367 - loss: 0.3965 - val_accuracy: 0.9041 - val_loss: 0.4203\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9404 - loss: 0.3846 - val_accuracy: 0.9041 - val_loss: 0.4100\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9454 - loss: 0.3735 - val_accuracy: 0.9070 - val_loss: 0.4002\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9454 - loss: 0.3630 - val_accuracy: 0.9090 - val_loss: 0.3909\n"
     ]
    }
   ],
   "source": [
    "# Entrenando y evaluando el modelo B sin usar el modelo A\n",
    "tf.random.set_seed(42)\n",
    "model_B = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcion 2_1: \n",
    "Vamos a reutilizar ahora el modelo A para transferir conocimiento al modelo B y entrenarlo. \n",
    "1. Carga del modelo guardado: carga el modelo preentrenado desde el archivo guardado. \n",
    "2. Transferencia de las capas a utilizar al segundo modelo: \n",
    "    + Crea un nuevo modelo llamado model_B_on_A utilizando la clase Sequential de Keras.\n",
    "    + Se toman todas las capas del modelo model_A excepto la última capa (utilizando model_A.layers[:-1])\n",
    "3. Se añade una ultima capa densa nueva con una neurona al modelo B: Esta capa tiene 1 neurona (ya que es una tarea de clasificación binaria) y utiliza la función de activación sigmoidal (\"sigmoid\"), que es comúnmente utilizada en la capa de salida para problemas de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = tf.keras.models.load_model(\"Resultados/my_model_A.keras\")\n",
    "# Se transfieren todas las capas menos la última del modelo A al modelo B\n",
    "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
    "#Se crea una nueva capa densa con 1 neurona y función de activación sigmoidal para la clasificación binaria\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcion 2_2: \n",
    "Creamos de nuevo el modelo model_B_on_A pero esta vez clonando el modelo A ya que sino al entrenar el modelo B el modelo A también se ve afectado:\n",
    "1. Clona el modelo A:\n",
    "    + Crea una copia exacta (model_A_clone) del modelo model_A utilizando la función clone_model. Esto crea un nuevo modelo con la misma arquitectura que model_A, pero sin compartir los mismos pesos.\n",
    "    + Establece los pesos de model_A_clone con los mismos pesos que tiene el modelo original model_A. Esto es esencial para hacer que model_A_clone tenga los mismos parámetros de peso que model_A, de modo que sea una copia exacta.\n",
    "    + Se realiza la transferencia de pesos preentrenados\n",
    "\n",
    "2. Se traslada al modelo model_B_on_A las capas clonadas y sus pesos (excepto la última).\n",
    "3. Se añade una última capa densa nueva con una neurona al modelo B: esta capa tiene 1 neurona (ya que es una tarea de clasificación binaria)\n",
    "4. Se congelan las capas preentrenadas: Es necesario iterar sobre todas las capas de model_B_on_A excepto las agregadas por nosotros y establecer la propiedad trainable de cada capa en False. Evita que los pesos de esas capas se actualicen durante las primeras etapas del ajuste fino.\n",
    "\n",
    "La congelación y descongelación de capas es una estrategia para controlar cuándo y cómo se actualizan los pesos de las capas preentrenadas durante el proceso de entrenamiento. Esto puede ser beneficioso cuando se tiene un conjunto de datos limitado para la tarea específica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Se congela todas las capas del modelo A\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.4350 - loss: 1.3379 - val_accuracy: 0.4797 - val_loss: 0.8819\n",
      "Epoch 2/4\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3883 - loss: 0.9160 - val_accuracy: 0.4570 - val_loss: 0.7658\n",
      "Epoch 3/4\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3524 - loss: 0.8123 - val_accuracy: 0.4570 - val_loss: 0.7314\n",
      "Epoch 4/4\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3803 - loss: 0.7738 - val_accuracy: 0.4906 - val_loss: 0.7077\n"
     ]
    }
   ],
   "source": [
    "# Se compila el modelo B sobre el modelo A con la función de pérdida binary_crossentropy, el optimizador SGD con learning_rate=0.001 y la métrica accuracy\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# Se entrena el modelo B sobre el modelo A con 4 épocas y los datos de validación B \n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descongelación de capas preentrenadas: se vuelve a cambiar la propiedad trainable a True para las capas preentrenadas. Esto significa que ahora las capas preentrenadas pueden aprender a partir de los datos de la tarea específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ponen todas las capas del modelo B sobre el modelo A como entrenables, lo que permitirá recarcular los pesos\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilación y entrenamiento final: Se compila y entrena el modelo completo (con todas las capas, incluidas las preentrenadas y las añadidas para la tarea específica) durante más épocas. Esto permite que el modelo ajuste finamente los pesos de todas las capas para la tarea específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.4888 - loss: 0.7325 - val_accuracy: 0.6271 - val_loss: 0.6335\n",
      "Epoch 2/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6300 - loss: 0.6506 - val_accuracy: 0.7428 - val_loss: 0.5718\n",
      "Epoch 3/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7122 - loss: 0.5826 - val_accuracy: 0.7953 - val_loss: 0.5230\n",
      "Epoch 4/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8152 - loss: 0.5295 - val_accuracy: 0.8388 - val_loss: 0.4845\n",
      "Epoch 5/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8560 - loss: 0.4871 - val_accuracy: 0.8694 - val_loss: 0.4532\n",
      "Epoch 6/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8810 - loss: 0.4526 - val_accuracy: 0.8912 - val_loss: 0.4271\n",
      "Epoch 7/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9053 - loss: 0.4239 - val_accuracy: 0.9001 - val_loss: 0.4049\n",
      "Epoch 8/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9161 - loss: 0.3995 - val_accuracy: 0.9080 - val_loss: 0.3857\n",
      "Epoch 9/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9261 - loss: 0.3784 - val_accuracy: 0.9130 - val_loss: 0.3688\n",
      "Epoch 10/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9405 - loss: 0.3599 - val_accuracy: 0.9120 - val_loss: 0.3540\n",
      "Epoch 11/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9442 - loss: 0.3437 - val_accuracy: 0.9159 - val_loss: 0.3409\n",
      "Epoch 12/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9479 - loss: 0.3293 - val_accuracy: 0.9219 - val_loss: 0.3291\n",
      "Epoch 13/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9565 - loss: 0.3165 - val_accuracy: 0.9248 - val_loss: 0.3185\n",
      "Epoch 14/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9565 - loss: 0.3050 - val_accuracy: 0.9248 - val_loss: 0.3090\n",
      "Epoch 15/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9565 - loss: 0.2945 - val_accuracy: 0.9248 - val_loss: 0.3003\n",
      "Epoch 16/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9565 - loss: 0.2850 - val_accuracy: 0.9278 - val_loss: 0.2923\n"
     ]
    }
   ],
   "source": [
    "# Se compila de nuevo el modelo B sobre el modelo A con la función de pérdida binary_crossentropy, el optimizador SGD con learning_rate=0.001 y la métrica accuracy\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# Se entrena el modelo B sobre el modelo A con 16 épocas y los datos de validación B\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 6: Evaluación el modelo con los datos de testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos ambos modelos para comparar la diferencia de utilizar la transferencia de modelos preentrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - accuracy: 0.9016 - loss: 0.3988\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.9336 - loss: 0.2973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3008052408695221, 0.9284999966621399]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluando el modelo B  sobre el conjunto de test \n",
    "model_B.evaluate(X_test_B, y_test_B)\n",
    "\n",
    "# Evaluando el modelo B sobre el modelo A  sobre el conjunto de test \n",
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sapa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
